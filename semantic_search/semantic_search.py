#!/usr/bin/env python3
"""
semantic_search - A self-documenting CLI tool for semantic file search using embeddings.

This tool enables semantic search across files and folders using sentence-transformers
for generating embeddings and FAISS for efficient similarity search. All processing
is done locally without requiring any external API.

Features:
    - Create and manage local search indexes
    - Search files semantically using natural language queries
    - Filter by file types (*.py, *.md, etc.)
    - Support for incremental index updates
    - Configurable index storage location

Exit Codes:
    0 - Success
    1 - Index not found
    2 - No results found
    3 - Invalid arguments or other errors

Environment Variables:
    SEMANTIC_SEARCH_DIR - Custom directory for storing indexes (default: ~/.semantic_search)

Examples:
    # Create an index for source code
    semantic_search index ./src --name my-code

    # Create an index for documentation with file type filter
    semantic_search index ./docs --name project-docs --include "*.md"

    # Search for authentication-related code
    semantic_search "authentication middleware" --index my-code --top-k 5

    # Search in documentation
    semantic_search "how to handle errors" --index project-docs

    # Quick search without creating a persistent index
    semantic_search "database connection" --path ./src

    # List all available indexes
    semantic_search list-indexes

    # Delete an index
    semantic_search delete-index my-code

    # Update an existing index with changed files
    semantic_search update-index my-code

Author: Generated by Claude
License: MIT
"""

import argparse
import fnmatch
import hashlib
import json
import os
import pickle
import sys
import textwrap
import time
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple

# Exit codes
EXIT_SUCCESS = 0
EXIT_INDEX_NOT_FOUND = 1
EXIT_NO_RESULTS = 2
EXIT_INVALID_ARGS = 3

# Default configuration
DEFAULT_MODEL = "all-MiniLM-L6-v2"
DEFAULT_CHUNK_SIZE = 512
DEFAULT_CHUNK_OVERLAP = 50
DEFAULT_TOP_K = 10

# File extensions considered as text files by default
DEFAULT_TEXT_EXTENSIONS = {
    '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.c', '.cpp', '.h', '.hpp',
    '.go', '.rs', '.rb', '.php', '.swift', '.kt', '.scala', '.r', '.m',
    '.md', '.txt', '.rst', '.adoc', '.org',
    '.json', '.yaml', '.yml', '.toml', '.ini', '.cfg', '.conf',
    '.html', '.htm', '.css', '.scss', '.sass', '.less',
    '.xml', '.xsl', '.xslt',
    '.sh', '.bash', '.zsh', '.fish', '.ps1', '.bat', '.cmd',
    '.sql', '.graphql', '.gql',
    '.dockerfile', '.makefile', '.cmake',
    '.gitignore', '.env', '.editorconfig',
}


def get_index_dir() -> Path:
    """Get the directory for storing semantic search indexes.

    Uses SEMANTIC_SEARCH_DIR environment variable if set,
    otherwise defaults to ~/.semantic_search/

    Returns:
        Path: The index storage directory
    """
    env_dir = os.environ.get('SEMANTIC_SEARCH_DIR')
    if env_dir:
        return Path(env_dir).expanduser().resolve()
    return Path.home() / '.semantic_search'


@dataclass
class FileChunk:
    """Represents a chunk of text from a file.

    Attributes:
        file_path: Absolute path to the source file
        content: The text content of the chunk
        start_line: Starting line number in the original file
        end_line: Ending line number in the original file
        chunk_index: Index of this chunk within the file
    """
    file_path: str
    content: str
    start_line: int
    end_line: int
    chunk_index: int


@dataclass
class IndexMetadata:
    """Metadata for a semantic search index.

    Attributes:
        name: Name of the index
        source_path: Original path that was indexed
        created_at: ISO format timestamp of creation
        updated_at: ISO format timestamp of last update
        model_name: Name of the embedding model used
        include_patterns: List of glob patterns for included files
        exclude_patterns: List of glob patterns for excluded files
        file_count: Number of files in the index
        chunk_count: Number of chunks in the index
        file_hashes: Dictionary mapping file paths to their content hashes
    """
    name: str
    source_path: str
    created_at: str
    updated_at: str
    model_name: str
    include_patterns: List[str] = field(default_factory=list)
    exclude_patterns: List[str] = field(default_factory=list)
    file_count: int = 0
    chunk_count: int = 0
    file_hashes: Dict[str, str] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert metadata to dictionary for JSON serialization."""
        return asdict(self)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'IndexMetadata':
        """Create metadata instance from dictionary."""
        return cls(**data)


class SemanticSearchIndex:
    """Manages a semantic search index.

    This class handles creating, loading, saving, and searching
    semantic indexes using sentence-transformers and FAISS.

    Attributes:
        name: Name of the index
        index_dir: Directory where index files are stored
        metadata: Index metadata
        model: SentenceTransformer model for embeddings
        faiss_index: FAISS index for similarity search
        chunks: List of FileChunk objects
    """

    def __init__(self, name: str, index_dir: Optional[Path] = None):
        """Initialize a semantic search index.

        Args:
            name: Name of the index
            index_dir: Custom directory for storing index files
        """
        self.name = name
        self.index_dir = (index_dir or get_index_dir()) / name
        self.metadata: Optional[IndexMetadata] = None
        self.model = None
        self.faiss_index = None
        self.chunks: List[FileChunk] = []

    def _load_model(self, model_name: str = DEFAULT_MODEL):
        """Load the sentence-transformers model.

        Args:
            model_name: Name of the model to load from HuggingFace
        """
        if self.model is None:
            try:
                from sentence_transformers import SentenceTransformer
                self.model = SentenceTransformer(model_name)
            except ImportError:
                print("Error: sentence-transformers not installed.", file=sys.stderr)
                print("Install with: pip install sentence-transformers", file=sys.stderr)
                sys.exit(EXIT_INVALID_ARGS)

    def _get_file_hash(self, file_path: Path) -> str:
        """Calculate MD5 hash of a file's content.

        Args:
            file_path: Path to the file

        Returns:
            MD5 hash as hexadecimal string
        """
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(8192), b''):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except (IOError, OSError):
            return ""

    def _is_text_file(self, file_path: Path) -> bool:
        """Check if a file is likely a text file.

        Args:
            file_path: Path to check

        Returns:
            True if the file appears to be a text file
        """
        # Check extension first
        if file_path.suffix.lower() in DEFAULT_TEXT_EXTENSIONS:
            return True

        # Check for files without extension (like Makefile, Dockerfile)
        if file_path.name.lower() in {'makefile', 'dockerfile', 'vagrantfile', 'jenkinsfile'}:
            return True

        # Try to detect binary files
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(8192)
                if b'\x00' in chunk:
                    return False
                # Check for high ratio of non-printable characters
                text_chars = bytearray({7, 8, 9, 10, 12, 13, 27} | set(range(0x20, 0x100)))
                non_text = sum(1 for byte in chunk if byte not in text_chars)
                return non_text / len(chunk) < 0.1 if chunk else True
        except (IOError, OSError):
            return False

    def _matches_patterns(self, file_path: Path, include_patterns: List[str],
                          exclude_patterns: List[str]) -> bool:
        """Check if a file matches include/exclude patterns.

        Args:
            file_path: Path to check
            include_patterns: List of glob patterns to include (empty = include all)
            exclude_patterns: List of glob patterns to exclude

        Returns:
            True if the file should be included
        """
        name = file_path.name
        rel_path = str(file_path)

        # Check exclude patterns first
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(name, pattern) or fnmatch.fnmatch(rel_path, pattern):
                return False

        # If no include patterns, include all (non-excluded) files
        if not include_patterns:
            return True

        # Check include patterns
        for pattern in include_patterns:
            if fnmatch.fnmatch(name, pattern) or fnmatch.fnmatch(rel_path, pattern):
                return True

        return False

    def _chunk_file(self, file_path: Path, chunk_size: int = DEFAULT_CHUNK_SIZE,
                    overlap: int = DEFAULT_CHUNK_OVERLAP) -> List[FileChunk]:
        """Split a file into overlapping chunks.

        Args:
            file_path: Path to the file
            chunk_size: Maximum number of lines per chunk
            overlap: Number of overlapping lines between chunks

        Returns:
            List of FileChunk objects
        """
        chunks = []
        try:
            with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
                lines = f.readlines()
        except (IOError, OSError) as e:
            print(f"Warning: Could not read {file_path}: {e}", file=sys.stderr)
            return chunks

        if not lines:
            return chunks

        # Calculate actual chunk size in lines (aim for ~512 tokens, roughly 4 chars per token)
        avg_line_len = sum(len(line) for line in lines) / len(lines)
        lines_per_chunk = max(10, int((chunk_size * 4) / max(1, avg_line_len)))
        overlap_lines = max(2, lines_per_chunk // 10)

        start = 0
        chunk_index = 0
        while start < len(lines):
            end = min(start + lines_per_chunk, len(lines))
            content = ''.join(lines[start:end]).strip()

            if content:  # Only add non-empty chunks
                chunks.append(FileChunk(
                    file_path=str(file_path.resolve()),
                    content=content,
                    start_line=start + 1,  # 1-indexed
                    end_line=end,
                    chunk_index=chunk_index
                ))
                chunk_index += 1

            start = end - overlap_lines if end < len(lines) else end

        return chunks

    def _collect_files(self, source_path: Path, include_patterns: List[str],
                       exclude_patterns: List[str]) -> List[Path]:
        """Collect all files matching the patterns from the source path.

        Args:
            source_path: Root directory to search
            include_patterns: Glob patterns for files to include
            exclude_patterns: Glob patterns for files to exclude

        Returns:
            List of file paths
        """
        files = []
        source_path = source_path.resolve()

        if source_path.is_file():
            if self._is_text_file(source_path):
                files.append(source_path)
        else:
            for root, dirs, filenames in os.walk(source_path):
                # Skip hidden directories and common non-code directories
                dirs[:] = [d for d in dirs if not d.startswith('.')
                          and d not in {'node_modules', '__pycache__', 'venv',
                                       '.git', '.svn', 'dist', 'build', 'target'}]

                for filename in filenames:
                    if filename.startswith('.'):
                        continue

                    file_path = Path(root) / filename
                    if (self._is_text_file(file_path) and
                        self._matches_patterns(file_path, include_patterns, exclude_patterns)):
                        files.append(file_path)

        return sorted(files)

    def create(self, source_path: Path, include_patterns: List[str] = None,
               exclude_patterns: List[str] = None, model_name: str = DEFAULT_MODEL,
               verbose: bool = False) -> int:
        """Create a new index from the source path.

        Args:
            source_path: Directory or file to index
            include_patterns: Glob patterns for files to include
            exclude_patterns: Glob patterns for files to exclude
            model_name: Name of the embedding model to use
            verbose: Print progress information

        Returns:
            Number of chunks indexed
        """
        import numpy as np
        try:
            import faiss
        except ImportError:
            print("Error: faiss-cpu not installed.", file=sys.stderr)
            print("Install with: pip install faiss-cpu", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

        include_patterns = include_patterns or []
        exclude_patterns = exclude_patterns or []
        source_path = source_path.resolve()

        if verbose:
            print(f"Loading model: {model_name}...")
        self._load_model(model_name)

        if verbose:
            print(f"Collecting files from: {source_path}")
        files = self._collect_files(source_path, include_patterns, exclude_patterns)

        if not files:
            print(f"No matching files found in {source_path}", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

        if verbose:
            print(f"Found {len(files)} files")

        # Chunk all files
        self.chunks = []
        file_hashes = {}
        for i, file_path in enumerate(files):
            if verbose:
                print(f"  [{i+1}/{len(files)}] Processing: {file_path.name}")
            file_hashes[str(file_path)] = self._get_file_hash(file_path)
            self.chunks.extend(self._chunk_file(file_path))

        if not self.chunks:
            print("No content found to index", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

        if verbose:
            print(f"Created {len(self.chunks)} chunks")
            print("Generating embeddings...")

        # Generate embeddings
        texts = [chunk.content for chunk in self.chunks]
        embeddings = self.model.encode(texts, show_progress_bar=verbose)

        # Create FAISS index
        dimension = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)
        self.faiss_index.add(embeddings.astype(np.float32))

        # Create metadata
        now = datetime.now().isoformat()
        self.metadata = IndexMetadata(
            name=self.name,
            source_path=str(source_path),
            created_at=now,
            updated_at=now,
            model_name=model_name,
            include_patterns=include_patterns,
            exclude_patterns=exclude_patterns,
            file_count=len(files),
            chunk_count=len(self.chunks),
            file_hashes=file_hashes
        )

        # Save the index
        self._save()

        if verbose:
            print(f"Index '{self.name}' created successfully!")
            print(f"  Files: {len(files)}")
            print(f"  Chunks: {len(self.chunks)}")
            print(f"  Location: {self.index_dir}")

        return len(self.chunks)

    def _save(self):
        """Save the index to disk."""
        import faiss

        self.index_dir.mkdir(parents=True, exist_ok=True)

        # Save metadata
        metadata_path = self.index_dir / 'metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(self.metadata.to_dict(), f, indent=2)

        # Save FAISS index
        faiss_path = self.index_dir / 'index.faiss'
        faiss.write_index(self.faiss_index, str(faiss_path))

        # Save chunks
        chunks_path = self.index_dir / 'chunks.pkl'
        with open(chunks_path, 'wb') as f:
            pickle.dump(self.chunks, f)

    def load(self) -> bool:
        """Load an existing index from disk.

        Returns:
            True if the index was loaded successfully
        """
        try:
            import faiss
        except ImportError:
            print("Error: faiss-cpu not installed.", file=sys.stderr)
            print("Install with: pip install faiss-cpu", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

        if not self.index_dir.exists():
            return False

        try:
            # Load metadata
            metadata_path = self.index_dir / 'metadata.json'
            with open(metadata_path, 'r') as f:
                self.metadata = IndexMetadata.from_dict(json.load(f))

            # Load FAISS index
            faiss_path = self.index_dir / 'index.faiss'
            self.faiss_index = faiss.read_index(str(faiss_path))

            # Load chunks
            chunks_path = self.index_dir / 'chunks.pkl'
            with open(chunks_path, 'rb') as f:
                self.chunks = pickle.load(f)

            # Load the model
            self._load_model(self.metadata.model_name)

            return True
        except (IOError, json.JSONDecodeError, pickle.UnpicklingError) as e:
            print(f"Error loading index: {e}", file=sys.stderr)
            return False

    def search(self, query: str, top_k: int = DEFAULT_TOP_K) -> List[Tuple[FileChunk, float]]:
        """Search the index for relevant chunks.

        Args:
            query: Natural language search query
            top_k: Number of results to return

        Returns:
            List of (FileChunk, score) tuples sorted by relevance
        """
        import numpy as np
        import faiss

        if self.faiss_index is None or not self.chunks:
            return []

        # Encode the query
        query_embedding = self.model.encode([query])
        faiss.normalize_L2(query_embedding)

        # Search
        k = min(top_k, len(self.chunks))
        scores, indices = self.faiss_index.search(query_embedding.astype(np.float32), k)

        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx >= 0 and idx < len(self.chunks):
                results.append((self.chunks[idx], float(score)))

        return results

    def update(self, verbose: bool = False) -> Tuple[int, int, int]:
        """Update the index with changed files.

        Returns:
            Tuple of (added, updated, removed) file counts
        """
        if not self.metadata:
            raise ValueError("No metadata loaded")

        source_path = Path(self.metadata.source_path)
        include_patterns = self.metadata.include_patterns
        exclude_patterns = self.metadata.exclude_patterns

        # Get current files
        current_files = self._collect_files(source_path, include_patterns, exclude_patterns)
        current_file_set = {str(f) for f in current_files}
        old_file_set = set(self.metadata.file_hashes.keys())

        # Find changes
        added = current_file_set - old_file_set
        removed = old_file_set - current_file_set

        # Check for modified files
        modified = set()
        for file_path in current_file_set & old_file_set:
            current_hash = self._get_file_hash(Path(file_path))
            if current_hash != self.metadata.file_hashes.get(file_path, ''):
                modified.add(file_path)

        files_to_reindex = added | modified

        if verbose:
            print(f"Files added: {len(added)}")
            print(f"Files modified: {len(modified)}")
            print(f"Files removed: {len(removed)}")

        if not (added or modified or removed):
            if verbose:
                print("No changes detected")
            return (0, 0, 0)

        # Re-create the entire index (simpler than partial updates with FAISS)
        if verbose:
            print("Re-indexing...")

        self.create(
            source_path,
            include_patterns=include_patterns,
            exclude_patterns=exclude_patterns,
            model_name=self.metadata.model_name,
            verbose=verbose
        )

        return (len(added), len(modified), len(removed))

    def delete(self) -> bool:
        """Delete the index from disk.

        Returns:
            True if the index was deleted successfully
        """
        import shutil

        if not self.index_dir.exists():
            return False

        try:
            shutil.rmtree(self.index_dir)
            return True
        except (IOError, OSError) as e:
            print(f"Error deleting index: {e}", file=sys.stderr)
            return False


def quick_search(query: str, path: Path, include_patterns: List[str] = None,
                 exclude_patterns: List[str] = None, top_k: int = DEFAULT_TOP_K,
                 model_name: str = DEFAULT_MODEL, verbose: bool = False) -> List[Tuple[FileChunk, float]]:
    """Perform a quick search without creating a persistent index.

    Args:
        query: Search query
        path: Directory or file to search
        include_patterns: File patterns to include
        exclude_patterns: File patterns to exclude
        top_k: Number of results to return
        model_name: Embedding model to use
        verbose: Print progress information

    Returns:
        List of (FileChunk, score) tuples
    """
    # Create a temporary index
    temp_index = SemanticSearchIndex('_temp_search')
    temp_index.index_dir = Path('/tmp') / '.semantic_search_temp'

    try:
        temp_index.create(
            path,
            include_patterns=include_patterns or [],
            exclude_patterns=exclude_patterns or [],
            model_name=model_name,
            verbose=verbose
        )
        return temp_index.search(query, top_k)
    finally:
        # Clean up
        temp_index.delete()


def list_indexes() -> List[IndexMetadata]:
    """List all available indexes.

    Returns:
        List of IndexMetadata objects
    """
    index_dir = get_index_dir()
    indexes = []

    if not index_dir.exists():
        return indexes

    for subdir in index_dir.iterdir():
        if subdir.is_dir():
            metadata_path = subdir / 'metadata.json'
            if metadata_path.exists():
                try:
                    with open(metadata_path, 'r') as f:
                        indexes.append(IndexMetadata.from_dict(json.load(f)))
                except (IOError, json.JSONDecodeError):
                    pass

    return sorted(indexes, key=lambda x: x.name)


def format_result(chunk: FileChunk, score: float, show_content: bool = True) -> str:
    """Format a search result for display.

    Args:
        chunk: The file chunk
        score: Similarity score
        show_content: Whether to show the content preview

    Returns:
        Formatted string
    """
    lines = [
        f"\n{'='*60}",
        f"File: {chunk.file_path}",
        f"Lines: {chunk.start_line}-{chunk.end_line}",
        f"Score: {score:.4f}",
    ]

    if show_content:
        # Truncate long content
        content = chunk.content
        if len(content) > 500:
            content = content[:500] + "..."
        lines.append(f"\n{'-'*40}")
        lines.append(content)

    return '\n'.join(lines)


def create_parser() -> argparse.ArgumentParser:
    """Create the argument parser with comprehensive help.

    Returns:
        Configured ArgumentParser
    """
    # Main parser
    parser = argparse.ArgumentParser(
        prog='semantic_search',
        description=textwrap.dedent("""
            Semantic Search CLI - Search files using natural language queries.

            This tool uses sentence-transformers for generating embeddings and FAISS
            for efficient similarity search. All processing is done locally.
        """),
        epilog=textwrap.dedent("""
            Examples:
              %(prog)s index ./src --name my-code
                  Create an index named 'my-code' from the ./src directory

              %(prog)s index ./docs --name docs --include "*.md" "*.rst"
                  Create an index for documentation files only

              %(prog)s "authentication middleware" --index my-code
                  Search for authentication-related code in the my-code index

              %(prog)s "database connection" --path ./src --top-k 5
                  Quick search (no persistent index) with 5 results

              %(prog)s list-indexes
                  List all available indexes

              %(prog)s update-index my-code
                  Update the my-code index with changed files

              %(prog)s delete-index my-code
                  Delete the my-code index

            Environment Variables:
              SEMANTIC_SEARCH_DIR  Custom directory for storing indexes
                                   (default: ~/.semantic_search)

            Exit Codes:
              0  Success
              1  Index not found
              2  No results found
              3  Invalid arguments or other errors
        """),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    # Subparsers for commands
    subparsers = parser.add_subparsers(dest='command', help='Available commands')

    # Index command
    index_parser = subparsers.add_parser(
        'index',
        help='Create a new search index',
        description='Create a new semantic search index from files in the specified path.',
        epilog=textwrap.dedent("""
            Examples:
              %(prog)s ./src --name my-code
              %(prog)s ./docs --name docs --include "*.md"
              %(prog)s . --name project --exclude "*.log" "*.tmp"
        """),
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    index_parser.add_argument(
        'path',
        type=Path,
        help='Directory or file to index'
    )
    index_parser.add_argument(
        '--name', '-n',
        required=True,
        help='Name for the index (required)'
    )
    index_parser.add_argument(
        '--include', '-i',
        nargs='+',
        default=[],
        metavar='PATTERN',
        help='Glob patterns for files to include (e.g., "*.py" "*.md")'
    )
    index_parser.add_argument(
        '--exclude', '-e',
        nargs='+',
        default=[],
        metavar='PATTERN',
        help='Glob patterns for files to exclude'
    )
    index_parser.add_argument(
        '--model', '-m',
        default=DEFAULT_MODEL,
        help=f'Embedding model to use (default: {DEFAULT_MODEL})'
    )
    index_parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Show detailed progress'
    )

    # List indexes command
    list_parser = subparsers.add_parser(
        'list-indexes',
        help='List all available indexes',
        description='Display information about all created indexes.'
    )
    list_parser.add_argument(
        '--json',
        action='store_true',
        help='Output in JSON format'
    )

    # Delete index command
    delete_parser = subparsers.add_parser(
        'delete-index',
        help='Delete an existing index',
        description='Permanently delete an index and all its data.'
    )
    delete_parser.add_argument(
        'name',
        help='Name of the index to delete'
    )
    delete_parser.add_argument(
        '--force', '-f',
        action='store_true',
        help='Delete without confirmation'
    )

    # Update index command
    update_parser = subparsers.add_parser(
        'update-index',
        help='Update an existing index',
        description='Re-index changed files in an existing index.'
    )
    update_parser.add_argument(
        'name',
        help='Name of the index to update'
    )
    update_parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Show detailed progress'
    )

    # Search (default command - positional query)
    parser.add_argument(
        'query',
        nargs='?',
        help='Search query (natural language)'
    )
    parser.add_argument(
        '--index', '-x',
        metavar='NAME',
        help='Name of the index to search'
    )
    parser.add_argument(
        '--path', '-p',
        type=Path,
        metavar='PATH',
        help='Path to search directly (without using an index)'
    )
    parser.add_argument(
        '--include', '-i',
        nargs='+',
        default=[],
        metavar='PATTERN',
        help='Glob patterns for files to include (with --path)'
    )
    parser.add_argument(
        '--exclude', '-e',
        nargs='+',
        default=[],
        metavar='PATTERN',
        help='Glob patterns for files to exclude (with --path)'
    )
    parser.add_argument(
        '--top-k', '-k',
        type=int,
        default=DEFAULT_TOP_K,
        metavar='N',
        help=f'Number of results to return (default: {DEFAULT_TOP_K})'
    )
    parser.add_argument(
        '--model', '-m',
        default=DEFAULT_MODEL,
        help=f'Embedding model to use (default: {DEFAULT_MODEL})'
    )
    parser.add_argument(
        '--no-content',
        action='store_true',
        help='Do not show content preview in results'
    )
    parser.add_argument(
        '--json',
        action='store_true',
        dest='output_json',
        help='Output results in JSON format'
    )
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Show detailed progress'
    )
    parser.add_argument(
        '--version',
        action='version',
        version='%(prog)s 1.0.0'
    )

    return parser


def main():
    """Main entry point for the CLI."""
    parser = create_parser()
    args = parser.parse_args()

    # Handle subcommands
    if args.command == 'index':
        # Create new index
        if not args.path.exists():
            print(f"Error: Path not found: {args.path}", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

        index = SemanticSearchIndex(args.name)
        try:
            chunk_count = index.create(
                args.path,
                include_patterns=args.include,
                exclude_patterns=args.exclude,
                model_name=args.model,
                verbose=args.verbose
            )
            if not args.verbose:
                print(f"Index '{args.name}' created with {chunk_count} chunks")
            sys.exit(EXIT_SUCCESS)
        except Exception as e:
            print(f"Error creating index: {e}", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

    elif args.command == 'list-indexes':
        # List all indexes
        indexes = list_indexes()

        if args.json:
            print(json.dumps([idx.to_dict() for idx in indexes], indent=2))
        elif not indexes:
            print("No indexes found")
            print(f"\nCreate one with: semantic_search index <path> --name <name>")
        else:
            print(f"Found {len(indexes)} index(es):\n")
            for idx in indexes:
                print(f"  {idx.name}")
                print(f"    Source: {idx.source_path}")
                print(f"    Files: {idx.file_count}, Chunks: {idx.chunk_count}")
                print(f"    Model: {idx.model_name}")
                print(f"    Created: {idx.created_at}")
                if idx.include_patterns:
                    print(f"    Include: {', '.join(idx.include_patterns)}")
                print()
        sys.exit(EXIT_SUCCESS)

    elif args.command == 'delete-index':
        # Delete an index
        index = SemanticSearchIndex(args.name)

        if not index.index_dir.exists():
            print(f"Error: Index '{args.name}' not found", file=sys.stderr)
            sys.exit(EXIT_INDEX_NOT_FOUND)

        if not args.force:
            response = input(f"Delete index '{args.name}'? [y/N] ")
            if response.lower() not in ('y', 'yes'):
                print("Cancelled")
                sys.exit(EXIT_SUCCESS)

        if index.delete():
            print(f"Index '{args.name}' deleted")
            sys.exit(EXIT_SUCCESS)
        else:
            print(f"Error deleting index '{args.name}'", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

    elif args.command == 'update-index':
        # Update an existing index
        index = SemanticSearchIndex(args.name)

        if not index.load():
            print(f"Error: Index '{args.name}' not found", file=sys.stderr)
            sys.exit(EXIT_INDEX_NOT_FOUND)

        try:
            added, modified, removed = index.update(verbose=args.verbose)
            if added or modified or removed:
                print(f"Index '{args.name}' updated: {added} added, {modified} modified, {removed} removed")
            else:
                print(f"Index '{args.name}' is up to date")
            sys.exit(EXIT_SUCCESS)
        except Exception as e:
            print(f"Error updating index: {e}", file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

    else:
        # Search query (default behavior)
        if not args.query:
            parser.print_help()
            sys.exit(EXIT_SUCCESS)

        # Determine search mode
        if args.path:
            # Quick search (no persistent index)
            if not args.path.exists():
                print(f"Error: Path not found: {args.path}", file=sys.stderr)
                sys.exit(EXIT_INVALID_ARGS)

            results = quick_search(
                args.query,
                args.path,
                include_patterns=args.include,
                exclude_patterns=args.exclude,
                top_k=args.top_k,
                model_name=args.model,
                verbose=args.verbose
            )
        elif args.index:
            # Search using existing index
            index = SemanticSearchIndex(args.index)

            if not index.load():
                print(f"Error: Index '{args.index}' not found", file=sys.stderr)
                print(f"\nAvailable indexes:", file=sys.stderr)
                for idx in list_indexes():
                    print(f"  - {idx.name}", file=sys.stderr)
                sys.exit(EXIT_INDEX_NOT_FOUND)

            results = index.search(args.query, args.top_k)
        else:
            print("Error: Must specify either --index or --path for search", file=sys.stderr)
            print("\nExamples:", file=sys.stderr)
            print('  semantic_search "query" --index my-index', file=sys.stderr)
            print('  semantic_search "query" --path ./src', file=sys.stderr)
            sys.exit(EXIT_INVALID_ARGS)

        # Output results
        if not results:
            print("No results found")
            sys.exit(EXIT_NO_RESULTS)

        if args.output_json:
            output = []
            for chunk, score in results:
                output.append({
                    'file_path': chunk.file_path,
                    'start_line': chunk.start_line,
                    'end_line': chunk.end_line,
                    'score': score,
                    'content': chunk.content if not args.no_content else None
                })
            print(json.dumps(output, indent=2))
        else:
            print(f"Found {len(results)} result(s) for: {args.query}")
            for chunk, score in results:
                print(format_result(chunk, score, show_content=not args.no_content))

        sys.exit(EXIT_SUCCESS)


if __name__ == '__main__':
    main()
